num_users: 100 # number of total users
num_selected_users: 10 # number of selected users
round: 500 # rounds of training
tau: 1 # number of local epochs
batch_size: 128 # local batch size
optimizer: adamw
local_lr: 0.01 # local learning rate
decay_weight: 0.99 # learning rate decay weight
lr_step_size: 1

method: ffm_fedavg
attack: None

model_heterogeneity: depthffm_fim
heterogeneous_group: [1/3, 1/3, 1/3]
lora_layer: 12
heterogeneous_group0_lora: 6
heterogeneous_group1_lora: 9
heterogeneous_group2_lora: 12
layer_prob: [1/9, 1/9, 1/9, 2/27, 2/27, 1/27, 1/27, 1/27, 2/27, 1/9, 1/9, 1/9]

data_type: image

#####################################
########## non iid type #############
#####################################
noniid_type: dirichlet
########## pathological ########
pat_num_cls: 20 # 3 for cifar10
########## dirichlet ###########
dir_cls_alpha: 0.3 # 1.0 0.3
################################
##### Parition Distribution ####
################################
partition_mode: dir # uni, dir

#####################################
########### fim params ##############
fim_every_iter: 50
fim_prior_epoch: 50


#####################################
########### LoRA Configuration ######
#####################################
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.1
lora_target_modules: ["query", "value"]
lora_bias: "none"

#####################################
########### Training Configuration ##
#####################################
num_workers: 0  # Avoid multiprocessing issues in federated setting
shuffle_training: true
drop_last: true
shuffle_eval: false
drop_last_eval: false
logging_batches: 3  # Number of batches to log during training
eval_batches: 3     # Number of batches to log during evaluation

#####################################
########### Model Configuration #####
#####################################
vit_base_hidden_size: 768
vit_large_hidden_size: 1024
flattened_size_cifar: 150528  # 3*224*224 for CIFAR-100 with ViT

#####################################
########### Client Configuration ####
#####################################
server_address: "localhost"
server_port: 8080
client_id: 0
seed: 1
gpu_id: -1
log_level: "INFO"
training_steps_per_epoch: 10
min_learning_rate: 0.001
default_learning_rate: 0.01
default_local_epochs: 1

#####################################
########### Non-IID Defaults ########
#####################################
default_noniid_type: 'dirichlet'
default_pat_num_cls: 10
default_partition_mode: 'dir'
default_dir_alpha: 0.5
default_dir_beta: 1.0


# Below seem unused in original FedHello code that is relevant to this experiment
#_base_ : ['../../base_ffm.yaml']
model: google/vit-base-patch16-224-in21k
peft: lora
iid: 0 # whether i.i.d or not
dataset: cifar100
local_momentum: 0.5 # SGD Momentum default 0.5
global_momentum: 0.9 # global momentum
optimizer_weight_decay: 0.0
defend: None
lr_scheduler_type: linear